project_name: autonomous_video_editor_best_ai
description: >
  Full multimodal AI pipeline for autonomous video editing.
  Teachers = maximum quality models, Students = distilled/efficient deployable models.

paths:
  cache_dir: ./cache
  data_dir: ./datasets
  output_dir: ./outputs

teachers:
  text_model: meta-llama/Llama-4-70b
  video_model: OpenGVLab/InternVideo2-Large
  audio_models:
    - openai/whisper-large-v3
    - Qwen/Qwen2-Audio
  fusion_model: Qwen/Qwen2.5-VL-72B

students:
  text_model: microsoft/phi-3-mini-4k-instruct
  video_model: VideoMAE/video-mae-small
  audio_models:
    - microsoft/wav2vec2-base
    - AudioMAE/audio-mae-small
  fusion_model: llava-hf/llava-v1.6-mistral-7b

distillation:
  strategies:
    - feature_matching   # Align internal embeddings
    - logit_matching     # Soft targets (probability distillation)
    - multimodal_alignment # Contrastive loss across text, video, audio
  temperature: 2.0
  alpha: 0.5   # trade-off between hard vs soft labels
  batch_size: 16
  learning_rate: 3e-5

finetuning:
  method: qlora   # parameter-efficient fine-tuning
  precision: bfloat16
  max_epochs: 5
  gradient_accumulation: 8
  save_every: 1000
  eval_every: 500

rlhf:
  reward_model: Qwen/Qwen2.5-7B
  dataset: openassistant/oasst2-preferences
  method: ppo
  rollout_batch_size: 32
  learning_rate: 1e-6
  epochs: 3

datasets:
  video:
    - WebVid10M
    - ActivityNet-Captions
  audio:
    - AudioSet
    - Librispeech
  text:
    - openassistant/oasst1
    - Anthropic/hh-rlhf
  multimodal:
    - MSRVTT
    - HowTo100M
    - COCO-Captions

evaluation:
  benchmarks:
    - videoqa
    - video_captioning
    - asr
    - multimodal_reasoning
  metrics:
    - bleu
    - rouge
    - cider
    - wer
    - accuracy
