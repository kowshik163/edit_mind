project_name: autonomous_video_editor_best_ai
description: >
  Full multimodal AI pipeline for autonomous video editing.
  Teachers = maximum quality models, Students = distilled/efficient deployable models.

paths:
  cache_dir: ./cache
  data_dir: ./datasets
  output_dir: ./outputs

teachers:
  text_model: zai-org/GLM-4.5  # Replaced with a top-performing, open-source agentic LLM.
  video_model: OpenGVLab/InternVideo2-Large
  audio_models:
    - openai/whisper-large-v3
    - Qwen/Qwen2-Audio
  fusion_model: Qwen/Qwen2.5-VL-72B
  
  # Advanced teacher models for enhanced distillation
  object_detection: RT-DETR/rtdetr-resnet50  # Real-time object detection
  segmentation: HQ-SAM/sam-hq-vit-h  # High-quality segmentation
  music_analysis: BeatNet/beatnet  # Music structure and rhythm
  audio_separation: Demucs/htdemucs  # Audio source separation
  
  # Self-coding teacher model
  code_generation: codellama/CodeLlama-13b-Python-hf

students:
  text_model: microsoft/phi-3-mini-4k-instruct
  video_model: VideoMAE/video-mae-small
  audio_models:
    - microsoft/wav2vec2-base
    - AudioMAE/audio-mae-small
  fusion_model: llava-hf/llava-v1.6-mistral-7b

distillation:
  strategies:
    - feature_matching   # Align internal embeddings
    - logit_matching     # Soft targets (probability distillation)  
    - multimodal_alignment # Contrastive loss across text, video, audio
    - spatial_understanding # RT-DETR object detection knowledge
    - segmentation_knowledge # HQ-SAM fine boundary understanding  
    - rhythmic_understanding # BeatNet music structure knowledge
    - audio_separation # Demucs source separation knowledge
  temperature: 2.0
  alpha: 0.5   # trade-off between hard vs soft labels
  batch_size: 16
  learning_rate: 3e-5
  
  # Enhanced distillation settings
  advanced_teachers:
    enable_rt_detr: true
    enable_hq_sam: true  
    enable_beatnet: true
    enable_demucs: true
    fallback_models: true  # Use fallback implementations if advanced models unavailable

finetuning:
  method: qlora   # parameter-efficient fine-tuning
  precision: bfloat16
  max_epochs: 5
  gradient_accumulation: 8
  save_every: 1000
  eval_every: 500

rlhf:
  reward_model: Qwen/Qwen2.5-7B
  dataset: openassistant/oasst2-preferences
  method: ppo
  rollout_batch_size: 32
  learning_rate: 1e-6

# Enhanced self-coding configuration
self_coding:
  model_name: codellama/CodeLlama-13b-Python-hf  # Upgraded to a more capable CodeLlama model
  device: auto  # auto-detect CUDA/CPU
  max_length: 1024
  
  # Fine-tuning configuration for video effects
  fine_tuning:
    enable: true
    output_dir: ./checkpoints/codellama-video-effects
    num_epochs: 3
    batch_size: 4
    learning_rate: 2e-5
    warmup_steps: 100
    
    # Training data sources
    data_sources:
      - video_effects_scripts
      - professional_editing
      - curated_examples
      
  # Safe code execution
  execution:
    max_execution_time: 30  # seconds
    max_temp_files: 10
    allowed_modules:
      - numpy
      - cv2
      - PIL
      - moviepy.editor
      - subprocess
      - tempfile
    
    # Multi-step execution capabilities  
    enable_multi_step: true
    enable_ffmpeg_integration: true
    enable_asset_generation: true
  epochs: 3

datasets:
  video:
    - WebVid10M
    - ActivityNet-Captions
  audio:
    - AudioSet
    - Librispeech
  text:
    - openassistant/oasst1
    - Anthropic/hh-rlhf
  multimodal:
    - MSRVTT
    - HowTo100M
    - COCO-Captions

evaluation:
  benchmarks:
    - videoqa
    - video_captioning
    - asr
    - multimodal_reasoning
  metrics:
    - bleu
    - rouge
    - cider
    - wer
    - accuracy
