project_name: autonomous_video_editor_best_ai
description: >
  Full multimodal AI pipeline for autonomous video editing.
  Teachers = maximum quality models, Students = distilled/efficient deployable models.

paths:
  cache_dir: ./cache
  data_dir: ./datasets
  output_dir: ./outputs

teachers:
  text_model: zai-org/GLM-4.5  # Replaced with a top-performing, open-source agentic LLM.
  video_model: OpenGVLab/InternVideo2-Large
  audio_models:
    - openai/whisper-large-v3
    - Qwen/Qwen2-Audio
  fusion_model: Qwen/Qwen2.5-VL-72B
  
  # Advanced teacher models for enhanced distillation
  object_detection: RT-DETR/rtdetr-resnet50  # Real-time object detection
  segmentation: HQ-SAM/sam-hq-vit-h  # High-quality segmentation
  music_analysis: BeatNet/beatnet  # Music structure and rhythm
  audio_separation: Demucs/htdemucs  # Audio source separation
  
  # New advanced video generation models
  video_generation:
    wan: alibaba-pai/Wan-2.2  # Alibaba's Wan 2.2 video generation
    mochi: genmo/mochi-1  # Genmo's Mochi 1 video model
    ltx_video: lightricks/ltx-video  # Lightricks LTX-Video
    hunyuan_video: tencent/hunyuan-video  # Tencent HunyuanVideo
    videocrafter: VideoCrafter/VideoCrafter1  # VideoCrafter1
  
  # Self-coding teacher model
  code_generation: codellama/CodeLlama-13b-Python-hf

students:
  text_model: microsoft/phi-3-mini-4k-instruct
  video_model: VideoMAE/video-mae-small
  audio_models:
    - microsoft/wav2vec2-base
    - AudioMAE/audio-mae-small
  fusion_model: llava-hf/llava-v1.6-mistral-7b

# Model architecture configuration
model:
  text_dim: 768
  vision_dim: 768
  audio_dim: 768
  fusion_dim: 1024
  hidden_dim: 2048
  num_attention_heads: 16
  num_layers: 12

distillation:
  strategies:
    - feature_matching   # Align internal embeddings
    - logit_matching     # Soft targets (probability distillation)  
    - multimodal_alignment # Contrastive loss across text, video, audio
    - spatial_understanding # RT-DETR object detection knowledge
    - segmentation_knowledge # HQ-SAM fine boundary understanding  
    - rhythmic_understanding # BeatNet music structure knowledge
    - audio_separation # Demucs source separation knowledge
  temperature: 2.0
  alpha: 0.5   # trade-off between hard vs soft labels
  batch_size: 16
  learning_rate: 3e-5
  
  # Enhanced distillation settings
  advanced_teachers:
    enable_rt_detr: true
    enable_hq_sam: true  
    enable_beatnet: true
    enable_demucs: true
    fallback_models: true  # Use fallback implementations if advanced models unavailable

finetuning:
  method: qlora   # parameter-efficient fine-tuning
  precision: bfloat16
  max_epochs: 5
  gradient_accumulation: 8
  save_every: 1000
  eval_every: 500

rlhf:
  reward_model: Qwen/Qwen2.5-7B
  dataset: openassistant/oasst2-preferences
  method: ppo
  rollout_batch_size: 32
  learning_rate: 1e-6

# Enhanced self-coding configuration
self_coding:
  model_name: codellama/CodeLlama-13b-Python-hf  # Upgraded to a more capable CodeLlama model
  device: auto  # auto-detect CUDA/CPU
  max_length: 1024
  
  # Fine-tuning configuration for video effects
  fine_tuning:
    enable: true
    output_dir: ./checkpoints/codellama-video-effects
    num_epochs: 3
    batch_size: 4
    learning_rate: 2e-5
    warmup_steps: 100
    
    # Training data sources
    data_sources:
      - video_effects_scripts
      - professional_editing
      - curated_examples
      
  # Safe code execution
  execution:
    max_execution_time: 30  # seconds
    max_temp_files: 10
    allowed_modules:
      - numpy
      - cv2
      - PIL
      - moviepy.editor
      - subprocess
      - tempfile
    
    # Multi-step execution capabilities  
    enable_multi_step: true
    enable_ffmpeg_integration: true
    enable_asset_generation: true
  epochs: 3

datasets:
  video:
    - WebVid10M
    - ActivityNet-Captions
    - UCF101  # Action recognition dataset
    - Kinetics-400  # Large-scale video action dataset
  audio:
    - AudioSet
    - Librispeech
  text:
    - openassistant/oasst1
    - Anthropic/hh-rlhf
  multimodal:
    - MSRVTT
    - HowTo100M
    - COCO-Captions
  
  # Template datasets for video editing training
  templates:
    mixkit:
      url: "https://mixkit.co"
      description: "Free Premiere Pro and After Effects templates"
      categories: ["intros", "transitions", "effects", "lower_thirds", "titles"]
      formats: ["aep", "prproj", "mogrt"]
    videezy:
      url: "https://videezy.com"
      description: "Free video templates and motion graphics"
      categories: ["motion_graphics", "video_templates", "overlays"]
      formats: ["aep", "prproj", "mov", "mp4"]
    capcut_templates:
      description: "TikTok-style editing templates including phonk beats"
      categories: ["transitions", "effects", "beat_sync", "phonk", "viral_edits"]
      source: "capcut_api"
    motion_array_free:
      url: "https://motionarray.com/browse/free"
      description: "Professional-grade free templates"
      categories: ["premiere_pro", "after_effects", "davinci_resolve"]
    canva_video:
      url: "https://canva.com/video-templates"
      description: "Simple video templates with text overlays"
      categories: ["text_animations", "social_media", "presentations"]
    youtube_creator_packs:
      description: "Community-contributed template packs"
      sources: ["youtube_descriptions", "creator_channels", "github_repos"]
  
  # Stock footage for template pairing
  stock_footage:
    pixabay:
      url: "https://pixabay.com/videos"
      description: "Free stock video clips"
    pexels:
      url: "https://pexels.com/videos"  
      description: "Free stock videos"
    videvo:
      url: "https://videvo.net"
      description: "Free stock footage and motion graphics"

evaluation:
  benchmarks:
    - videoqa
    - video_captioning
    - asr
    - multimodal_reasoning
  metrics:
    - bleu
    - rouge
    - cider
    - wer
    - accuracy
