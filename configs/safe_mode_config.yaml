project_name: autonomous_video_editor_test_run
description: Minimal configuration for testing the pipeline on a local machine.

paths:
  cache_dir: ./cache
  data_dir: ./data/synthetic_samples  # Point to the local data we will create
  output_dir: ./outputs

# Use small, widely available models instead of massive ones
teachers:
  text_model: microsoft/DialoGPT-small
  video_model: openai/clip-vit-base-patch32
  audio_models:
    - openai/whisper-tiny
  code_generation: codellama/CodeLlama-7b-hf # Smallest CodeLlama

students:
  text_model: microsoft/DialoGPT-small
  video_model: openai/clip-vit-base-patch32
  audio_models:
    - openai/whisper-tiny

# Model architecture configuration
model:
  text_dim: 768
  vision_dim: 768
  audio_dim: 768
  fusion_dim: 1024
  hidden_dim: 2048
  num_attention_heads: 16
  num_layers: 12
# Training configuration
training:
  batch_size: 8
  
  # Phase 1: Fusion Pretraining
  phase1:
    num_epochs: 3
    learning_rate: 1e-4
    weight_decay: 0.01
    gradient_accumulation_steps: 2
    warmup_steps: 100
    
  # Phase 2: Knowledge Distillation  
  phase2:
    num_epochs: 3
    learning_rate: 3e-5
    weight_decay: 0.01
    gradient_accumulation_steps: 4
    
  # Phase 3: Editing Fine-tuning
  phase3:
    num_epochs: 5
    learning_rate: 1e-5
    weight_decay: 0.01
    gradient_accumulation_steps: 8

# Data configuration
data:
  max_frames: 32
  max_audio_length: 160000
  frame_sample_rate: 2

# Disable or minimize complex training phases
distillation:
  strategies:
    - feature_matching
  temperature: 2.0
  alpha: 0.5
  batch_size: 1 # Use a batch size of 1 for minimal memory usage
  learning_rate: 1e-5

# Training configuration for safe mode
training:
  # Training configuration
  batch_size: 8

  # Phase 1: Fusion Pretraining
  phase1:
    num_epochs: 3
    learning_rate: 1e-4
    weight_decay: 0.01
    gradient_accumulation_steps: 2
    warmup_steps: 100

  # Phase 2: Knowledge Distillation
  phase2:
    num_epochs: 3
    learning_rate: 3e-5
    weight_decay: 0.01
    gradient_accumulation_steps: 4

  # Phase 3: Editing Fine-tuning
  phase3:
    num_epochs: 5
    learning_rate: 1e-5
    weight_decay: 0.01
    gradient_accumulation_steps: 8
  
  phases: ['pretrain', 'distill']  # Skip complex phases for safe mode
  batch_size: 2
  learning_rate: 1e-4
  num_epochs: 1
  gradient_accumulation_steps: 1
  warmup_steps: 10
  eval_steps: 10
  save_steps: 20
  max_grad_norm: 1.0

# Data configuration for safe mode  
data:
  datasets: []  # Start with no datasets, use synthetic only
  max_samples_per_dataset: 50
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1
  num_workers: 1

finetuning:
  method: qlora
  max_epochs: 1
  gradient_accumulation: 1
  save_every: 100
  eval_every: 50

# Disable RLHF and Self-Coding for the test run
rlhf:
  enabled: false
self_coding:
  enabled: false

# Crucially, remove all large online datasets
datasets:
  templates:
    enabled: false
  stock_footage:
    enabled: false

evaluation:
  metrics:
    - bleu
    - rouge